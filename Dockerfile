FROM python:3.11-slim AS base

ARG SPARK_VERSION=3.5.4
ARG HADOOP_VERSION=3
ENV SPARK_HOME=/opt/spark
ENV PATH=$SPARK_HOME/bin:$SPARK_HOME/sbin:$PATH \
    PYTHONHASHSEED=1 \
    SPARK_LOG_DIR="$SPARK_HOME/logs" \
    SPARK_MASTER_LOG="$SPARK_HOME/logs/spark-master.out" \
    SPARK_WORKER_LOG="$SPARK_HOME/logs/spark-worker.out"

# Installer Java pour Spark
RUN apt-get update && apt-get install -y wget curl \
    openjdk-17-jdk libz-dev libssl-dev \
    && apt-get clean && rm -rf /var/lib/apt/lists/*

RUN wget --no-verbose -O apache-spark.tgz "https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz" && \
    mkdir -p /opt/spark && \
    tar -xf apache-spark.tgz -C /opt/spark --strip-components=1 && \
    rm apache-spark.tgz

## Installer les dÃ©pendances Python
#COPY requirements.txt /app/requirements.txt
RUN pip install --no-cache-dir pandas

WORKDIR $SPARK_HOME

RUN mkdir -p $SPARK_LOG_DIR && \
    touch $SPARK_MASTER_LOG && \
    touch $SPARK_WORKER_LOG && \
    ln -sf /dev/stdout $SPARK_MASTER_LOG && \
    ln -sf /dev/stdout $SPARK_WORKER_LOG

COPY start-spark.sh /

RUN echo 'alias l="ls -lA --color --group-directories-first"' >> /root/.bashrc

EXPOSE 8080 7077 4040
CMD ["/bin/bash", "/start-spark.sh"]
